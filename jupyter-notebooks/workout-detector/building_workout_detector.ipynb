{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workout Detector\n",
    "### The models built here will use keypoints extracted from PoseNet as features and the type of exercise as the label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()\n",
    "\n",
    "train_dataset = pd.read_csv(\"./training_set.csv\")\n",
    "test_dataset = pd.read_csv(\"./test_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the keypoints into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_keypoints_to_np(keypoint_string):\n",
    "    numbers = re.findall(\"\\[\\s*-*\\d+.\\d+\\s+-*\\d+.\\d+\\s*\\]\", keypoint_string)\n",
    "    \n",
    "    new_num_arr = []\n",
    "    for num_arr in numbers:\n",
    "        convert_to_array = list(map(lambda x: float(x), re.findall(\"\\d+.\\d+\", num_arr)))\n",
    "        new_num_arr.append(convert_to_array)\n",
    "    new_num_arr\n",
    "    \n",
    "    return np.array(new_num_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[\"keypoints\"] = train_dataset[\"keypoints\"].transform(lambda x: convert_keypoints_to_np(x))\n",
    "test_dataset[\"keypoints\"] = test_dataset[\"keypoints\"].transform(lambda x: convert_keypoints_to_np(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode labels as 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_dataset[\"label\"])\n",
    "\n",
    "train_dataset[\"label\"] = le.transform(train_dataset[\"label\"])\n",
    "test_dataset[\"label\"] = le.transform(test_dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop invalid keypoint rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_invalid_keypoint_rows(dataset, dataset_type):\n",
    "    missing_count = 0\n",
    "    missing_indices = []\n",
    "    for i in range(0, len(dataset[\"keypoints\"])):\n",
    "        if str(dataset[\"keypoints\"][i].shape) != \"(17, 2)\":\n",
    "            missing_count += 1\n",
    "            missing_indices.append(i)\n",
    "\n",
    "    print(\"The {} data had {} rows with invalid keypoints\".format(dataset_type, missing_count))\n",
    "    dataset = dataset.drop(dataset.index[missing_indices]).reset_index(drop=True)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Train data had 0 rows with invalid keypoints\n",
      "The Test data had 0 rows with invalid keypoints\n"
     ]
    }
   ],
   "source": [
    "# Drop any rows with keypoints that we couldn't retrieve for training data\n",
    "train_dataset = drop_invalid_keypoint_rows(train_dataset, \"Train\")\n",
    "\n",
    "# Drop any rows with keypoints that we couldn't retrieve for test data\n",
    "test_dataset = drop_invalid_keypoint_rows(test_dataset, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack keypoints to create 3D Feature vectors and create Train and Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_keypoints(dataset, dataset_type):\n",
    "    features = dataset[\"keypoints\"].to_numpy()\n",
    "    features = np.stack(features, axis=0)\n",
    "    print(\"Dimensions of the {} features: {}\".format(dataset_type, features.shape))\n",
    "\n",
    "    labels = dataset[\"label\"].to_numpy()\n",
    "    print(\"Dimensions of the {} labels: {}\".format(dataset_type, labels.shape))\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the Train features: (758, 17, 2)\n",
      "Dimensions of the Train labels: (758,)\n",
      "Dimensions of the Test features: (492, 17, 2)\n",
      "Dimensions of the Test labels: (492,)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = stack_keypoints(train_dataset, \"Train\")\n",
    "X_test, Y_test = stack_keypoints(test_dataset, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an SVM on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_svc(x_train, x_test, y_train, y_test):\n",
    "    # Shuffle the data\n",
    "    x_train, y_train = shuffle(x_train, y_train, random_state=0)\n",
    "    x_test, y_test = shuffle(x_test, y_test, random_state=0)\n",
    "    \n",
    "    # Scale the data using Standard Scalar\n",
    "    scalers = {}\n",
    "    for i in range(x_train.shape[2]):\n",
    "        scalers[i] = StandardScaler()\n",
    "        x_train[:, :, i] = scalers[i].fit_transform(x_train[:, :, i]) \n",
    "        x_test[:, :, i] = scalers[i].transform(x_test[:, :, i]) \n",
    "    \n",
    "    # Reshape the data\n",
    "    x_train = x_train.reshape((x_train.shape[0], x_train.shape[1] * x_train.shape[2]))\n",
    "    x_test = x_test.reshape((x_test.shape[0], x_test.shape[1] * x_test.shape[2]))\n",
    "    print(\"Dimensions of the train set after modifying: {} and {}\".format(x_train.shape, y_train.shape))\n",
    "    print(\"Dimensions of the test set after modifying: {} and {}\".format(x_test.shape, y_test.shape))\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    svc = LinearSVC(random_state=0, max_iter=100000).fit(x_train, y_train)\n",
    "    print(\"The train accuracy is {}\".format(svc.score(x_train, y_train)))\n",
    "    print(\"The test accuracy is: {}\".format(svc.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the train set after modifying: (758, 34) and (758,)\n",
      "Dimensions of the test set after modifying: (492, 34) and (492,)\n",
      "The train accuracy is 1.0\n",
      "The test accuracy is: 0.4166666666666667\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_svc(X_train, X_test, Y_train, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
