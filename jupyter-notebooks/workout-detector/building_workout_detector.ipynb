{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workout Detector\n",
    "### The models built here will use keypoints extracted from PoseNet as features and the type of exercise as the label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()\n",
    "\n",
    "train_dataset = pd.read_csv(\"./training_set.csv\")\n",
    "test_dataset = pd.read_csv(\"./test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7ff06faf56a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the keypoints into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_keypoints_to_np(keypoint_string):\n",
    "    numbers = re.findall(\"\\[\\s*-*\\d+.\\d+\\s+-*\\d+.\\d+\\s*\\]\", keypoint_string)\n",
    "    \n",
    "    new_num_arr = []\n",
    "    for num_arr in numbers:\n",
    "        convert_to_array = list(map(lambda x: float(x), re.findall(\"\\d+.\\d+\", num_arr)))\n",
    "        new_num_arr.append(convert_to_array)\n",
    "    new_num_arr\n",
    "    \n",
    "    return np.array(new_num_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[\"keypoints\"] = train_dataset[\"keypoints\"].transform(lambda x: convert_keypoints_to_np(x))\n",
    "test_dataset[\"keypoints\"] = test_dataset[\"keypoints\"].transform(lambda x: convert_keypoints_to_np(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode labels as 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_dataset[\"label\"])\n",
    "\n",
    "train_dataset[\"label\"] = le.transform(train_dataset[\"label\"])\n",
    "test_dataset[\"label\"] = le.transform(test_dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop invalid keypoint rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_invalid_keypoint_rows(dataset, dataset_type):\n",
    "    missing_count = 0\n",
    "    missing_indices = []\n",
    "    for i in range(0, len(dataset[\"keypoints\"])):\n",
    "        if str(dataset[\"keypoints\"][i].shape) != \"(17, 2)\":\n",
    "            missing_count += 1\n",
    "            missing_indices.append(i)\n",
    "\n",
    "    print(\"The {} data had {} rows with invalid keypoints\".format(dataset_type, missing_count))\n",
    "    dataset = dataset.drop(dataset.index[missing_indices]).reset_index(drop=True)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Train data had 138 rows with invalid keypoints\n",
      "The Test data had 11 rows with invalid keypoints\n"
     ]
    }
   ],
   "source": [
    "# Drop any rows with keypoints that we couldn't retrieve for training data\n",
    "train_dataset = drop_invalid_keypoint_rows(train_dataset, \"Train\")\n",
    "\n",
    "# Drop any rows with keypoints that we couldn't retrieve for test data\n",
    "test_dataset = drop_invalid_keypoint_rows(test_dataset, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack keypoints to create 3D Feature vectors and create Train and Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_keypoints(dataset, dataset_type):\n",
    "    features = dataset[\"keypoints\"].to_numpy()\n",
    "    features = np.stack(features, axis=0)\n",
    "    print(\"Dimensions of the {} features: {}\".format(dataset_type, features.shape))\n",
    "\n",
    "    labels = dataset[\"label\"].to_numpy()\n",
    "    print(\"Dimensions of the {} labels: {}\".format(dataset_type, labels.shape))\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the Train features: (19613, 17, 2)\n",
      "Dimensions of the Train labels: (19613,)\n",
      "Dimensions of the Test features: (5261, 17, 2)\n",
      "Dimensions of the Test labels: (5261,)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = stack_keypoints(train_dataset, \"Train\")\n",
    "X_test, Y_test = stack_keypoints(test_dataset, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an SVM on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_svc(x_train, x_test, y_train, y_test):\n",
    "    # Shuffle the data\n",
    "    x_train, y_train = shuffle(x_train, y_train, random_state=0)\n",
    "    x_test, y_test = shuffle(x_test, y_test, random_state=0)\n",
    "    \n",
    "    # Scale the data using Standard Scalar\n",
    "    scalers = {}\n",
    "    for i in range(x_train.shape[2]):\n",
    "        scalers[i] = StandardScaler()\n",
    "        x_train[:, :, i] = scalers[i].fit_transform(x_train[:, :, i]) \n",
    "        x_test[:, :, i] = scalers[i].transform(x_test[:, :, i]) \n",
    "    \n",
    "    # Reshape the data\n",
    "    x_train = x_train.reshape((x_train.shape[0], x_train.shape[1] * x_train.shape[2]))\n",
    "    x_test = x_test.reshape((x_test.shape[0], x_test.shape[1] * x_test.shape[2]))\n",
    "    print(\"Dimensions of the train set after modifying: {} and {}\".format(x_train.shape, y_train.shape))\n",
    "    print(\"Dimensions of the test set after modifying: {} and {}\".format(x_test.shape, y_test.shape))\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    svc = LinearSVC(random_state=0, max_iter=100000).fit(x_train, y_train)\n",
    "    print(\"The train accuracy is {}\".format(svc.score(x_train, y_train)))\n",
    "    print(\"The test accuracy is: {}\".format(svc.score(x_test, y_test)))\n",
    "    \n",
    "    # Save the scalers\n",
    "    x_scaler = scalers[0]\n",
    "    y_scaler = scalers[1]\n",
    "\n",
    "    return svc, x_scaler, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the train set after modifying: (19613, 34) and (19613,)\n",
      "Dimensions of the test set after modifying: (5261, 34) and (5261,)\n",
      "The train accuracy is 0.9964819252536583\n",
      "The test accuracy is: 0.9929671165177723\n",
      "{0: StandardScaler(copy=True, with_mean=True, with_std=True), 1: StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "model, x_scaler, y_scaler = train_and_evaluate_svc(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_workout_detector.joblib']"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(model, 'svm_workout_detector.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Standard Scaler for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y_scaler.save']"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the x scaler \n",
    "x_scaler\n",
    "dump(x_scaler, \"x_scaler.save\") \n",
    "\n",
    "# Save the y scaler \n",
    "y_scaler\n",
    "dump(y_scaler, \"y_scaler.save\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of loading and using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = load('svm_workout_detector.joblib') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
